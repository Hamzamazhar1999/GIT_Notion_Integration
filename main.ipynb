{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ðŸ“¦ Repository: Qwen2.5-VL</span>\n",
       "â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ .gitignore</span> (93 bytes)\n",
       "â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ LICENSE</span> (11357 bytes)\n",
       "â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ README.md</span> (31843 bytes)\n",
       "â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ cookbooks</span>\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ assets</span>\n",
       "â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ agent_function_call</span>\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ mobile_en_example.png</span> (574101 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ mobile_zh_example.jpg</span> (230339 bytes)\n",
       "â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ computer_use</span>\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ computer_use1.jpeg</span> (769981 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ computer_use2.jpeg</span> (290406 bytes)\n",
       "â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ document_parsing</span>\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ docparsing_example1.jpg</span> (977153 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ docparsing_example2.jpg</span> (413487 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ docparsing_example3.jpg</span> (546105 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ docparsing_example4.jpg</span> (454123 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ docparsing_example5.png</span> (561834 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ docparsing_example6.png</span> (453201 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ docparsing_example7.jpg</span> (211360 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ docparsing_example8.png</span> (434990 bytes)\n",
       "â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ ocr</span>\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ .ipynb_checkpoints</span>\n",
       "â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr_example1-checkpoint.jpg</span> (796861 bytes)\n",
       "â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr_example2-checkpoint.jpg</span> (133470 bytes)\n",
       "â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr_example3-checkpoint.jpg</span> (206181 bytes)\n",
       "â”‚   â”‚   â”‚   â”‚   â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr_example4-checkpoint.jpg</span> (96595 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr_example1.jpg</span> (796861 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr_example2.jpg</span> (72651 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr_example3.jpg</span> (206181 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr_example4.jpg</span> (96595 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr_example5.jpg</span> (147224 bytes)\n",
       "â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ spatial_understanding</span>\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ Origamis.jpg</span> (2045134 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ cakes.png</span> (1783424 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ cartoon_brave_person.jpeg</span> (60134 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ multiple_items.png</span> (38610 bytes)\n",
       "â”‚   â”‚   â””â”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ universal_recognition</span>\n",
       "â”‚   â”‚       â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ unireco_bird_example.jpg</span> (676514 bytes)\n",
       "â”‚   â”‚       â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ unireco_birds_example.jpg</span> (360686 bytes)\n",
       "â”‚   â”‚       â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ unireco_celebrities_example.jpg</span> (137771 bytes)\n",
       "â”‚   â”‚       â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ unireco_landmarks_example.jpg</span> (646537 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ computer_use.ipynb</span> (9532352 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ document_parsing.ipynb</span> (1974037 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ mobile_agent.ipynb</span> (2802874 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ ocr.ipynb</span> (3259249 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ spatial_understanding.ipynb</span> (6423721 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ universal_recognition.ipynb</span> (2279737 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ utils</span>\n",
       "â”‚   â”‚   â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ agent_function_call.py</span> (11389 bytes)\n",
       "â”‚   â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ video_understanding.ipynb</span> (2031733 bytes)\n",
       "â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ docker</span>\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ Dockerfile-cu121</span> (1995 bytes)\n",
       "â”‚   â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ docker_web_demo.sh</span> (1835 bytes)\n",
       "â”œâ”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ qwen-vl-utils</span>\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ .python-version</span> (7 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ README.md</span> (5373 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ pyproject.toml</span> (1641 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ requirements-dev.lock</span> (1736 bytes)\n",
       "â”‚   â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ requirements.lock</span> (635 bytes)\n",
       "â”‚   â””â”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ src</span>\n",
       "â”‚       â””â”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ qwen_vl_utils</span>\n",
       "â”‚           â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ __init__.py</span> (134 bytes)\n",
       "â”‚           â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ vision_process.py</span> (14652 bytes)\n",
       "â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ requirements_web_demo.txt</span> (310 bytes)\n",
       "â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ web_demo_mm.py</span> (11391 bytes)\n",
       "â””â”€â”€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ðŸ“‚ web_demo_streaming</span>\n",
       "    â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ app.py</span> (28536 bytes)\n",
       "    â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ global.js</span> (4752 bytes)\n",
       "    â”œâ”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ main.js</span> (2454 bytes)\n",
       "    â””â”€â”€ <span style=\"color: #008000; text-decoration-color: #008000\">ðŸ“„ recorder.js</span> (4974 bytes)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mðŸ“¦ Repository: Qwen2.5-VL\u001b[0m\n",
       "â”œâ”€â”€ \u001b[32mðŸ“„ .gitignore\u001b[0m (93 bytes)\n",
       "â”œâ”€â”€ \u001b[32mðŸ“„ LICENSE\u001b[0m (11357 bytes)\n",
       "â”œâ”€â”€ \u001b[32mðŸ“„ README.md\u001b[0m (31843 bytes)\n",
       "â”œâ”€â”€ \u001b[1;36mðŸ“‚ cookbooks\u001b[0m\n",
       "â”‚   â”œâ”€â”€ \u001b[1;36mðŸ“‚ assets\u001b[0m\n",
       "â”‚   â”‚   â”œâ”€â”€ \u001b[1;36mðŸ“‚ agent_function_call\u001b[0m\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ mobile_en_example.png\u001b[0m (574101 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ \u001b[32mðŸ“„ mobile_zh_example.jpg\u001b[0m (230339 bytes)\n",
       "â”‚   â”‚   â”œâ”€â”€ \u001b[1;36mðŸ“‚ computer_use\u001b[0m\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ computer_use1.jpeg\u001b[0m (769981 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ \u001b[32mðŸ“„ computer_use2.jpeg\u001b[0m (290406 bytes)\n",
       "â”‚   â”‚   â”œâ”€â”€ \u001b[1;36mðŸ“‚ document_parsing\u001b[0m\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ docparsing_example1.jpg\u001b[0m (977153 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ docparsing_example2.jpg\u001b[0m (413487 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ docparsing_example3.jpg\u001b[0m (546105 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ docparsing_example4.jpg\u001b[0m (454123 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ docparsing_example5.png\u001b[0m (561834 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ docparsing_example6.png\u001b[0m (453201 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ docparsing_example7.jpg\u001b[0m (211360 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ \u001b[32mðŸ“„ docparsing_example8.png\u001b[0m (434990 bytes)\n",
       "â”‚   â”‚   â”œâ”€â”€ \u001b[1;36mðŸ“‚ ocr\u001b[0m\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[1;36mðŸ“‚ .ipynb_checkpoints\u001b[0m\n",
       "â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ ocr_example1-checkpoint.jpg\u001b[0m (796861 bytes)\n",
       "â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ ocr_example2-checkpoint.jpg\u001b[0m (133470 bytes)\n",
       "â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ ocr_example3-checkpoint.jpg\u001b[0m (206181 bytes)\n",
       "â”‚   â”‚   â”‚   â”‚   â””â”€â”€ \u001b[32mðŸ“„ ocr_example4-checkpoint.jpg\u001b[0m (96595 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ ocr_example1.jpg\u001b[0m (796861 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ ocr_example2.jpg\u001b[0m (72651 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ ocr_example3.jpg\u001b[0m (206181 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ ocr_example4.jpg\u001b[0m (96595 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ \u001b[32mðŸ“„ ocr_example5.jpg\u001b[0m (147224 bytes)\n",
       "â”‚   â”‚   â”œâ”€â”€ \u001b[1;36mðŸ“‚ spatial_understanding\u001b[0m\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ Origamis.jpg\u001b[0m (2045134 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ cakes.png\u001b[0m (1783424 bytes)\n",
       "â”‚   â”‚   â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ cartoon_brave_person.jpeg\u001b[0m (60134 bytes)\n",
       "â”‚   â”‚   â”‚   â””â”€â”€ \u001b[32mðŸ“„ multiple_items.png\u001b[0m (38610 bytes)\n",
       "â”‚   â”‚   â””â”€â”€ \u001b[1;36mðŸ“‚ universal_recognition\u001b[0m\n",
       "â”‚   â”‚       â”œâ”€â”€ \u001b[32mðŸ“„ unireco_bird_example.jpg\u001b[0m (676514 bytes)\n",
       "â”‚   â”‚       â”œâ”€â”€ \u001b[32mðŸ“„ unireco_birds_example.jpg\u001b[0m (360686 bytes)\n",
       "â”‚   â”‚       â”œâ”€â”€ \u001b[32mðŸ“„ unireco_celebrities_example.jpg\u001b[0m (137771 bytes)\n",
       "â”‚   â”‚       â””â”€â”€ \u001b[32mðŸ“„ unireco_landmarks_example.jpg\u001b[0m (646537 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ computer_use.ipynb\u001b[0m (9532352 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ document_parsing.ipynb\u001b[0m (1974037 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ mobile_agent.ipynb\u001b[0m (2802874 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ ocr.ipynb\u001b[0m (3259249 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ spatial_understanding.ipynb\u001b[0m (6423721 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ universal_recognition.ipynb\u001b[0m (2279737 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[1;36mðŸ“‚ utils\u001b[0m\n",
       "â”‚   â”‚   â””â”€â”€ \u001b[32mðŸ“„ agent_function_call.py\u001b[0m (11389 bytes)\n",
       "â”‚   â””â”€â”€ \u001b[32mðŸ“„ video_understanding.ipynb\u001b[0m (2031733 bytes)\n",
       "â”œâ”€â”€ \u001b[1;36mðŸ“‚ docker\u001b[0m\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ Dockerfile-cu121\u001b[0m (1995 bytes)\n",
       "â”‚   â””â”€â”€ \u001b[32mðŸ“„ docker_web_demo.sh\u001b[0m (1835 bytes)\n",
       "â”œâ”€â”€ \u001b[1;36mðŸ“‚ qwen-vl-utils\u001b[0m\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ .python-version\u001b[0m (7 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ README.md\u001b[0m (5373 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ pyproject.toml\u001b[0m (1641 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ requirements-dev.lock\u001b[0m (1736 bytes)\n",
       "â”‚   â”œâ”€â”€ \u001b[32mðŸ“„ requirements.lock\u001b[0m (635 bytes)\n",
       "â”‚   â””â”€â”€ \u001b[1;36mðŸ“‚ src\u001b[0m\n",
       "â”‚       â””â”€â”€ \u001b[1;36mðŸ“‚ qwen_vl_utils\u001b[0m\n",
       "â”‚           â”œâ”€â”€ \u001b[32mðŸ“„ __init__.py\u001b[0m (134 bytes)\n",
       "â”‚           â””â”€â”€ \u001b[32mðŸ“„ vision_process.py\u001b[0m (14652 bytes)\n",
       "â”œâ”€â”€ \u001b[32mðŸ“„ requirements_web_demo.txt\u001b[0m (310 bytes)\n",
       "â”œâ”€â”€ \u001b[32mðŸ“„ web_demo_mm.py\u001b[0m (11391 bytes)\n",
       "â””â”€â”€ \u001b[1;36mðŸ“‚ web_demo_streaming\u001b[0m\n",
       "    â”œâ”€â”€ \u001b[32mðŸ“„ app.py\u001b[0m (28536 bytes)\n",
       "    â”œâ”€â”€ \u001b[32mðŸ“„ global.js\u001b[0m (4752 bytes)\n",
       "    â”œâ”€â”€ \u001b[32mðŸ“„ main.js\u001b[0m (2454 bytes)\n",
       "    â””â”€â”€ \u001b[32mðŸ“„ recorder.js\u001b[0m (4974 bytes)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise Repo files\n",
    "import requests\n",
    "from IPython.display import display,Markdown\n",
    "from github_pat import pat\n",
    "from repo_visualizer import visualize_repo\n",
    "from repo_visualizer import fetch_file_url\n",
    "\n",
    "# Define the URL of the repository\n",
    "repo_url = \"https://api.github.com/repos/QwenLM/Qwen2.5-VL/contents\"\n",
    "\n",
    "# Call the function to visualize the repository structure\n",
    "visualize_repo(repo_url, pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://api.github.com/repos/QwenLM/Qwen2.5-VL/contents/README.md?ref=main', 'https://api.github.com/repos/QwenLM/Qwen2.5-VL/contents/qwen-vl-utils/README.md?ref=main']\n"
     ]
    }
   ],
   "source": [
    "# Get file from repo and decode from base64 to utf-8\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "\n",
    "file_name=input(\"Input file name: \") \n",
    "fetched_file_urls = fetch_file_url(repo_url, file_name, pat)\n",
    "\n",
    "fetched_file_urls_index=0\n",
    "if (len(fetched_file_urls)) > 1:\n",
    "     print(fetched_file_urls)\n",
    "     fetched_file_urls_index = input(\"Input URL index:\")\n",
    "     assert fetched_file_urls_index.replace('.', '', 1).isdigit(), \"Error: Input should be a number.\"\n",
    "     fetched_file_urls_index = int(fetched_file_urls_index)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {pat}\"}\n",
    "response = requests.get(fetched_file_urls[fetched_file_urls_index], headers=headers)\n",
    "name, extension = os.path.splitext(file_name)\n",
    "\n",
    "with open(f\"{name}.json\", \"w\") as json_file:\n",
    "     json_file.write(json.dumps(response.json(), indent=4))\n",
    "\n",
    "with open(f\"{name}.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)  # Load JSON as a Python dictionary\n",
    "\n",
    "decoded_file = base64.b64decode(data[\"content\"]).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompt for gpt-4o-mini\n",
    "from openai import OpenAI\n",
    "from apikey import gpt_api\n",
    "\n",
    "prompt_template = lambda decoded_file: f\"\"\"\n",
    "You are a professional technical writer and summarization expert specializing in transforming complex documentation into concise, clear, and well-structured summaries. Your goal is to analyze the provided file from a repository and generate a professional summary.\n",
    "\n",
    "### Guidelines:\n",
    "1. **Relevance**:  \n",
    "   - Focus on the **core purpose** of the file.  \n",
    "   - Highlight key **features**, **functionalities**, and **intended use cases**.  \n",
    "   - Identify the **target audience** or **users** of the file.\n",
    "\n",
    "2. **Clarity**:  \n",
    "   - Use **simple and precise language** to explain the file's purpose and usage.  \n",
    "   - Avoid overly technical jargon unless necessary for understanding.  \n",
    "\n",
    "3. **Organization**:  \n",
    "   - Structure the summary with headings if appropriate (e.g., \"Overview\", \"Key Features\", \"Usage\").  \n",
    "   - Ensure the output is **well-formatted** and easy to read.  \n",
    "\n",
    "4. **Formatting**:  \n",
    "   - Output the summary in **Markdown format**.  \n",
    "\n",
    "5. **Important**:\n",
    "   - l.\n",
    "---\n",
    "\n",
    "### Input:\n",
    "- **file **:  \n",
    "{decoded_file}\n",
    "\n",
    "---\n",
    "\n",
    "### Output:  \n",
    "1. **Summary**:  \n",
    "   - A **concise and well-structured summary** of the file.  \n",
    "   - Highlights the file's **purpose**, **features**, and **usage**.  \n",
    "   - Organized with headings for clarity and readability.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prompt tokens\n",
    "\n",
    "prompt = prompt_template(decoded_file)\n",
    "from tiktoken import encoding_for_model\n",
    "import numpy as np\n",
    "\n",
    "tokens = encoding_for_model('gpt-4o-mini').encode(decoded_file)\n",
    "print(len(np.array(tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup api client\n",
    "client = OpenAI(api_key=gpt_api)\n",
    "\n",
    "# make api call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Expert software engineer summarization for files\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], \n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "# extract response\n",
    "response_string = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Qwen2.5-VL Summary\n",
       "\n",
       "## Overview\n",
       "Qwen2.5-VL is an advanced vision-language model designed to enhance interaction with visual and textual data. Building on the previous Qwen2-VL model, it introduces significant improvements in document parsing, video understanding, and object grounding, making it suitable for various applications involving multimodal data processing.\n",
       "\n",
       "## Key Features\n",
       "- **Document Parsing**: Enhanced capabilities for omnidocument parsing, allowing for effective processing of multi-scene, multilingual documents, including handwriting, tables, charts, and more.\n",
       "- **Object Grounding**: Improved accuracy in detecting and counting objects with support for absolute coordinates and JSON formats for advanced spatial reasoning.\n",
       "- **Video Understanding**: Capable of comprehending ultra-long videos with fine-grained event extraction, employing dynamic resolution in the temporal dimension.\n",
       "- **Agent Functionality**: Enhanced decision-making and reasoning capabilities for computer and mobile applications, optimizing interaction with visual data.\n",
       "\n",
       "## Model Architecture Updates\n",
       "- **Dynamic Resolution**: Introduces dynamic FPS sampling for video understanding, allowing the model to learn temporal sequences and pinpoint specific moments effectively.\n",
       "- **Efficient Vision Encoder**: Optimizations in the Vision Transformer (ViT) architecture improve training and inference speeds.\n",
       "\n",
       "## Performance\n",
       "Qwen2.5-VL models have shown superior performance across various benchmarks, outperforming previous models in several tasks, including document understanding and visual reasoning.\n",
       "\n",
       "## Quickstart Guide\n",
       "Users can easily integrate Qwen2.5-VL into their projects using the provided installation commands and example code snippets. Support for various input formats (images, video URLs, base64 encodings) facilitates flexible usage in applications.\n",
       "\n",
       "## Target Audience\n",
       "The Qwen2.5-VL model is intended for developers and researchers in the fields of machine learning, computer vision, and natural language processing looking to leverage multimodal capabilities in their applications.\n",
       "\n",
       "## Additional Resources\n",
       "- [Demo](https://huggingface.co/spaces/Qwen/Qwen2.5-VL)\n",
       "- [Cookbooks](https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks) for practical examples and use cases.\n",
       "- [API Documentation](https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api) for integrating the model into applications.\n",
       "\n",
       "For detailed performance metrics and further insights into the model's capabilities, please refer to the [official blog](https://qwenlm.github.io/blog/qwen2.5-vl/) and the provided GitHub repository."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
