{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof of Concept\n",
    "    -> URL scraping a GitHub repo files\n",
    "    -> Processing and linking an openAI API to summarise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Installing the required libraries using requirements.txt\n",
    "    -> pip install -r \".\\path\\to\\requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Deciding scraping method\n",
    "    -> We have two options here: 1) Using raw scraping 2) Using GitHub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# 2D-TLM\n",
       "Porting a 2D Transmission-Line Matrix Algorithm to CUDA\n",
       "\n",
       "TLM or Transmission Line matrix method is a numerical technique that employs the time domain\n",
       "to provide an approximation of the electromagnetic wave propagation. This approach uses a\n",
       "cartesian matrix of nodes to depict the two-dimensional space where wave propagation and\n",
       "scattering take place. It revolves around the discretization of the propagation of electromagnetic\n",
       "waves in both time and space. This is an iterative procedure with the two steps of scattering and\n",
       "connecting as its key components. TLM offers a highly instructive technique to use this algorithm\n",
       "in computer simulations and wave propagation modeling.\n",
       "\n",
       "The 2D CPU code provided as a reference for port to GPU has portions that could be very well\n",
       "parallelized and optimized for runtime speedups and optimized application. The first thing that is\n",
       "noticeable is the definition of compute extensive and repeating applications like the sqrt() function.\n",
       "\n",
       "The most important parts that can be parallelized are the scatter and connect functions. In a\n",
       "Transmission Line Matrix (TLM) simulation, the scatter function is used to update the grid of\n",
       "nodes at each time step. Based on the neighbors’ values and the TLM equation's coefficients, it\n",
       "calculates the new values for the nodes.\n",
       "\n",
       "The grid has the dimensions 'Nx x Ny'. Each node in the grid has four different port voltages: 'V1',\n",
       "'V2', 'V3', and 'V4'. The inner loop computes the updated values for the port voltages at each node\n",
       "while the outer loop iterates across the rows and columns of the grid. The port voltages\n",
       "and impedance 'Z' are used in calculation of the current 'I' flowing through the node. The current\n",
       "and impedances are then used to update the port voltages.\n",
       "The next part of the code that can be parallelized is the connect function. The structure of the\n",
       "simulated network is set using this function. It describes the connections between the nodes of the\n",
       "grids. The coefficients of the TLM equation are set up using the connect functions and these\n",
       "ultimately lead to the update of the node values.\n",
       "\n",
       "# Changes made in the GPU kernel:\n",
       "\n",
       "Initially, the connect function is used to exchange the V2 and V4 port voltage values between the\n",
       "grid nodes. Then the exchange of the voltages at the V1 and V3 port happens. These transactions\n",
       "establish the connection of the grid nodes.\n",
       "\n",
       "The nodes along the grid's edges are subject to boundary conditions according to the boundary\n",
       "function. It multiplies the V3 and V1 port voltages for the nodes at the top and bottom borders of\n",
       "the grid by the corresponding boundary reflection coefficients, rYmax and rYmin. It multiplies the\n",
       "V4 and V2 port voltages for the nodes at the left and right edges of the grid by the corresponding\n",
       "boundary reflection coefficients, rXmax and rXmin.\n",
       "\n",
       "Other than these two functions, the source and the output probing functions could also be done\n",
       "using CUDA kernels to ensure a seamless data allocation in device without having to copy and\n",
       "allocate new memory locations after the updates.\n",
       "\n",
       "Furthermore, there are some inconsistencies in the code that could be dealt with by using simple\n",
       "coding techniques to properly optimize memory allocation and memory access. These include\n",
       "dynamic allocation of Ein[] and Eout[] arrays to allow easy memory copying from host to device\n",
       "for further processing and evaluating the output\n",
       "\n",
       "# Improvements from GPU Use:\n",
       "![image](https://github.com/Hamzamazhar1999/2D-TLM/assets/129704102/75fcacc2-c3d9-41b9-aeb6-3f412f903fd9)\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Method 1 is using raw scarping\n",
    "# First, we have to create an HTTP request to the URL we wanna fetch from\n",
    "\n",
    "import requests\n",
    "from IPython.display import display,Markdown\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Hamzamazhar1999/2D-TLM/refs/heads/main/README.md\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code==200:\n",
    "    readme_data = response.text\n",
    "    display(Markdown(readme_data))\n",
    "    # Issue number 1 is embedded links to local locations will not show up since \n",
    "    # we are only fetching the raw of the markdown, if there are href links then\n",
    "    # we want to fetch them as well (this will complete the image/link missing\n",
    "    # stuff)\n",
    "else:\n",
    "    print(\"Error fetching URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Deciding scraping method\n",
    "    -> If using raw scraping, and gotten MD file, import transformers for summarization\n",
    "    -> Or other model APIs for contextual summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# A couple of issues with the current model pipeline from transformers is \n",
    "# that the model does not perform well for tokens over 1024, but long \n",
    "# md files will have a higher number of tokens, summarization of md \n",
    "# files is not done well by this naive model, plus the md files have\n",
    "# excessive formatting which confuses the model\n",
    "# Instead of using Summarization, I want to use another model to ensure correct summarization,\n",
    "# and that also allows for a higher number of tokens to be passed through  \n",
    "# summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# summary = summarizer(readme_data[:100], max_length=40)\n",
    "# print(summary[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from apikey import gpt_api\n",
    "\n",
    "prompt_template = lambda readme_data: f\"\"\"\n",
    "You are a professional technical writer and summarization expert specializing in transforming complex documentation into concise, clear, and well-structured summaries. Your goal is to analyze the provided README file from a repository and generate a professional summary.\n",
    "\n",
    "### Guidelines:\n",
    "1. **Relevance**:  \n",
    "   - Focus on the **core purpose** of the repository.  \n",
    "   - Highlight key **features**, **functionalities**, and **intended use cases**.  \n",
    "   - Identify the **target audience** or **users** of the repository.\n",
    "\n",
    "2. **Clarity**:  \n",
    "   - Use **simple and precise language** to explain the repository’s purpose and usage.  \n",
    "   - Avoid overly technical jargon unless necessary for understanding.  \n",
    "\n",
    "3. **Organization**:  \n",
    "   - Structure the summary with headings if appropriate (e.g., \"Overview\", \"Key Features\", \"Usage\").  \n",
    "   - Ensure the output is **well-formatted** and easy to read.  \n",
    "\n",
    "4. **Formatting**:  \n",
    "   - Output the summary in **Markdown format**.  \n",
    "\n",
    "5. **Important**:\n",
    "   - Make sure to not imagine and stay to the point, if the ReadMe.md file is small, keep the summary small.\n",
    "---\n",
    "\n",
    "### Input:\n",
    "- **README file (Markdown format)**:  \n",
    "{readme_data}\n",
    "\n",
    "---\n",
    "\n",
    "### Output:  \n",
    "1. **Summary**:  \n",
    "   - A **concise and well-structured summary** of the repository.  \n",
    "   - Highlights the repository’s **purpose**, **features**, and **usage**.  \n",
    "   - Organized with headings for clarity and readability.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template(readme_data)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup api client\n",
    "client = OpenAI(api_key=gpt_api)\n",
    "\n",
    "# make api call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Expert software engineer summarization for files\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], \n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "# extract response\n",
    "response_string = response.choices[0].message.content\n",
    "# print(response_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# 2D-TLM Repository Summary\n",
       "\n",
       "## Overview\n",
       "The **2D-TLM** repository focuses on porting a **2D Transmission-Line Matrix (TLM)** algorithm to **CUDA**. TLM is a numerical method utilized for approximating electromagnetic wave propagation in two-dimensional spaces through a Cartesian matrix of nodes. The algorithm's iterative process involves scattering and connecting nodes to simulate wave behavior effectively.\n",
       "\n",
       "## Key Features\n",
       "- **TLM Methodology**: Implements a time-domain approach for modeling electromagnetic wave propagation.\n",
       "- **CUDA Optimization**: Transforms existing CPU code to leverage GPU parallelization, aiming for enhanced runtime performance.\n",
       "- **Core Functions**:\n",
       "  - **Scatter Function**: Updates node values based on neighboring nodes and TLM coefficients.\n",
       "  - **Connect Function**: Establishes connections between grid nodes and sets up TLM coefficients for node value updates.\n",
       "- **Boundary Conditions Handling**: Applies specific reflection coefficients at grid edges to simulate realistic physical conditions.\n",
       "- **Memory Optimization**: Introduces dynamic memory allocation strategies to improve data handling and processing efficiency on GPUs.\n",
       "\n",
       "## Intended Use Cases\n",
       "This repository is designed for researchers and developers interested in:\n",
       "- Simulating electromagnetic wave behaviors using TLM methods.\n",
       "- Enhancing computational efficiency through GPU programming.\n",
       "- Exploring numerical modeling techniques in electromagnetic theory.\n",
       "\n",
       "By providing both the CUDA-optimized code and a reference CPU implementation, users can gain insights into the performance benefits of GPU computing with TLM simulations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call notion api to convert the markdown file to a notion document\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
